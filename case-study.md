# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникли 2 проблемы.
1) Необходимо было импортировать в базу файл с данными, чуть больше 32 мегабайт (100К записей).
   Рейк таск успешно работал на файлах размером пару мегабайт, но c большими файлами он работала слишком долго.
   Я решил исправить эту проблему, оптимизировав этот скрипт.

2) Страницы расписаний тоже формируются не эффективно и при росте количества записей начинают сильно тормозить.
   Я решил исправить эту проблему, оптимизировав sql-запросы.
   
## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:
время выполнения программы/запроса в секундах

Бюджет метрики для импорта large.json - 60 секунд.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась без теста. Поэтому необхожимо было написать тест, проверяющий корректность работы программы.
Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за ~10 секунд

Вот как я построил `feedback_loop`:
- написал тесты для проверки корректности работы рейк таски/контроллера
- измерил метрику: для рейк таски для example.json (10К записей) - 200 мс
- написал performance тесты для защиты от деградации производительности
- далее использовал эти же входные данные для дальнейшего профилирования и сравнения метрик

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
- rack-mini-profiler
- rails panel
- bullet
- explain запросов

Вот какие проблемы удалось найти и решить:

Ваша находка №1